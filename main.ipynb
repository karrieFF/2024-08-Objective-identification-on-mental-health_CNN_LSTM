{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from missingpy import MissForest #impute missing value\n",
    "from sklearn.preprocessing import MinMaxScaler #standardized data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#deep learning package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import preprocessing\n",
    "from model import CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_data_frame2 = pd.read_csv('impute_data_frame2.csv')\n",
    "select_survey = pd.read_csv('select_survey.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augumentation: for Fitbit only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(time_series, mean=0.0, stddev=1.0):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to a time series.\n",
    "     Options:\n",
    "     time_series (array-like): A time series to which noise is added.\n",
    "     mean (float): The average value of the noise. Default is 0.0.\n",
    "     stddev (float): Standard deviation of noise. Default is 1.0.\n",
    "\n",
    "     Returns:\n",
    "     noisy_series (np.array): Time series with added noise.\n",
    "     \"\"\"\n",
    "     # Gaussian noise generation\n",
    "    noise = np.random.normal(mean, stddev, len(time_series))\n",
    "\n",
    "    # Adding noise to the original time series\n",
    "    noisy_series = time_series + noise\n",
    "\n",
    "    return noisy_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augumentation = impute_data_frame2.copy()\n",
    "non_need_augumentation = ['survey_date', 'coach_date', 'date']\n",
    "data_augumentation.drop(columns = non_need_augumentation, inplace = True)\n",
    "\n",
    "total_ids = data_augumentation['Id'].unique()\n",
    "after_augumentation = pd.DataFrame()\n",
    "\n",
    "for id in total_ids:\n",
    "    individual_data = data_augumentation[data_augumentation['Id'] == id]\n",
    "    individual_data = individual_data.drop(columns = 'Id').copy()\n",
    "\n",
    "    augmented_data = individual_data.apply(lambda col: add_gaussian_noise(col, mean=0.0, stddev=0.05))\n",
    "    augmented_data['Id'] = id\n",
    "    after_augumentation = pd.concat([after_augumentation, augmented_data])\n",
    "\n",
    "after_augumentation[['survey_date', 'coach_date', 'date']] = impute_data_frame2[['survey_date', 'coach_date', 'date']]\n",
    "after_augumentation.set_index('Id', inplace = True)\n",
    "after_augumentation.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge fitbit data with survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category mental health, three levels\n",
    "# The average of the five responses were used for data analysis\n",
    "select_survey['mental_health_1'] = select_survey[['nervous_v1', 'down_v1', 'calm_v1','blue_v1', 'happy_v1']].mean(axis = 1)\n",
    "select_survey['mental_health_2'] = select_survey[['nervous_v2', 'down_v2', 'calm_v2','blue_v2', 'happy_v2']].mean(axis = 1)\n",
    "\n",
    "#determine the median of mental health > I want to make it to be multicategory: low, controllable, high\n",
    "mental_health1_median = select_survey['mental_health_1'].median()\n",
    "mental_health2_median = select_survey['mental_health_2'].median()\n",
    "\n",
    "#transfer the variable to category\n",
    "select_survey['mental_health_1'] = select_survey['mental_health_1'].apply(lambda value: 0 if value < mental_health1_median else 1)\n",
    "select_survey['mental_health_2'] = select_survey['mental_health_2'].apply(lambda value: 0 if value < mental_health1_median else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karri\\AppData\\Local\\Temp\\ipykernel_27244\\2603091222.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recode_demographic_variable['demographic'] = np.nan\n",
      "C:\\Users\\karri\\AppData\\Local\\Temp\\ipykernel_27244\\2603091222.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recode_demographic_variable['Id'] = select_survey['Id']\n"
     ]
    }
   ],
   "source": [
    "#recode demongraphic variable\n",
    "all_fitbit_id = data_augumentation['Id'].unique()\n",
    "\n",
    "var = ['demographics_race___1','demographics_race___2','demographics_race___3','demographics_race___4',\n",
    "       'demographics_race___5','demographics_race___6','demographics_race___7']\n",
    "recode_demographic_variable = select_survey[var]\n",
    "recode_demographic_variable['demographic'] = np.nan\n",
    "recode_demographic_variable['Id'] = select_survey['Id']\n",
    "\n",
    "#1: demographics_race___1 = 1\n",
    "#2: demographics_race___2 = 1\n",
    "#3: demographics_race___3 = 3\n",
    "#4: demographics_race___4 = 4\n",
    "#5: demographics_race___5 = 5\n",
    "#6: demographics_race___6 = 6\n",
    "#7: demographics_race___7 = 7\n",
    "#8: multiple_race = 8\n",
    "\n",
    "# Recoding logic\n",
    "for i in all_fitbit_id:\n",
    "    row = recode_demographic_variable[recode_demographic_variable['Id'] == i][var]\n",
    "    # Check if there are multiple races selected\n",
    "    if row.sum(axis=1).iloc[0] > 1:\n",
    "        recode_demographic_variable.loc[recode_demographic_variable['Id'] == i, 'demographic'] = 8  # Multiple races\n",
    "    elif row['demographics_race___1'].iloc[0] == 1:\n",
    "        recode_demographic_variable.loc[recode_demographic_variable['Id'] == i, 'demographic'] = 1\n",
    "    elif row['demographics_race___2'].iloc[0] == 1:\n",
    "        recode_demographic_variable.loc[recode_demographic_variable['Id'] == i, 'demographic'] = 2\n",
    "    elif row['demographics_race___3'].iloc[0] == 1:\n",
    "        recode_demographic_variable.loc[recode_demographic_variable['Id'] == i, 'demographic'] = 3\n",
    "    elif row['demographics_race___4'].iloc[0] == 1:\n",
    "        recode_demographic_variable.loc[recode_demographic_variable['Id'] == i, 'demographic'] = 4\n",
    "    elif row['demographics_race___5'].iloc[0] == 1:\n",
    "        recode_demographic_variable.loc[recode_demographic_variable['Id'] == i, 'demographic'] = 5\n",
    "    elif row['demographics_race___6'].iloc[0] == 1:\n",
    "        recode_demographic_variable.loc[recode_demographic_variable['Id'] == i, 'demographic'] = 6\n",
    "    elif row['demographics_race___7'].iloc[0] == 1:\n",
    "        recode_demographic_variable.loc[recode_demographic_variable['Id'] == i, 'demographic'] = 7\n",
    "\n",
    "select_survey['demographics_race'] =  recode_demographic_variable['demographic']\n",
    "\n",
    "used_variables = ['Id', 'current_status','demographics_age','demographics_sex','demographics_sexorient',\n",
    "                  'demographics_ethnicity', 'demographics_immigration','demographics_race',\n",
    "                  'demographics_education', 'demographics_sorority']\n",
    "\n",
    "select_data = select_survey[used_variables]\n",
    "\n",
    "#merge Fitbit data with survey data\n",
    "fitbit_survey = pd.merge(after_augumentation, select_data, on = 'Id', how = 'left')\n",
    "\n",
    "#move the date time\n",
    "fitbit_survey.set_index('date', inplace=True)\n",
    "fitbit_survey.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\missingpy\\missforest.py:528: UserWarning: No missing value located; returning original dataset.\n",
      "  warnings.warn(\"No missing value located; returning original \"\n"
     ]
    }
   ],
   "source": [
    "# impute time sequence data\n",
    "miss_imputation3 = preprocessing.impute_missing(fitbit_survey)\n",
    "#keep only one date\n",
    "impute_data_frame3 = miss_imputation3.copy()\n",
    "impute_data_frame3 = impute_data_frame3.reset_index().drop(columns = 'index')\n",
    "date = impute_data_frame3.iloc[:, 0]\n",
    "impute_data_frame3 = impute_data_frame3.drop(columns=['date'])\n",
    "impute_data_frame3['date'] = date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# academic day: weekday except break\n",
    "# non-academic day: weekend, fallbreak, winterbreak, spring semester, spring break, summer holiday)\n",
    "\n",
    "#weekend:\n",
    "#----10.07 saturaday - non-academic day\n",
    "#----10.08 sunday - non-academic day\n",
    "\n",
    "#fall break \"2023-10-8 :2023-10-15\"\n",
    "#winter break '2023-11-15:'2023-01-08'\n",
    "#martine luda:  2024 January 15\n",
    "#president date:  2024 February 19\n",
    "#spring break: 2024 March 3-10\n",
    "#summer break: '2023-5-1: 2023-8-21'\n",
    "\n",
    "fall_break = pd.date_range(start='2023-10-8', end='2023-10-15')\n",
    "winter_break = pd.date_range(start='2023-11-15', end='2024-01-08')\n",
    "other_days = ['2024-01-15', '2024-02-19']\n",
    "other_days = pd.DatetimeIndex(pd.to_datetime(other_days))\n",
    "spring_break = pd.date_range(start='2024-03-03', end='2024-03-10')\n",
    "summer_break = pd.date_range(start='2023-5-1', end='2023-8-21')\n",
    "\n",
    "\n",
    "earlist_date = impute_data_frame3['date'].min()\n",
    "latest_date = impute_data_frame3['date'].max()\n",
    "\n",
    "# Generate a date range between the start and end dates\n",
    "determine_weekend = pd.date_range(start=earlist_date, end=latest_date)\n",
    "\n",
    "date_info = pd.DataFrame({\n",
    "    'date': determine_weekend,\n",
    "    'is_weekend': determine_weekend.weekday.isin([5, 6])  # True for Saturday (5) and Sunday (6)\n",
    "})\n",
    "\n",
    "# Filter to display only weekend days\n",
    "weekends = date_info[date_info['is_weekend']]['date'].values\n",
    "weekends = pd.DatetimeIndex(pd.to_datetime(weekends))\n",
    "\n",
    "#combine all the non-academic day\n",
    "non_academic_days = fall_break.union(winter_break).union(other_days).union(spring_break).union(summer_break).union(weekends)\n",
    "\n",
    "#recode the date data\n",
    "impute_data_frame3['date_code'] = impute_data_frame3['date'].apply(lambda x:0 if x in non_academic_days else 1)\n",
    "\n",
    "fitbit_survey2 = impute_data_frame3.drop(columns = 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_health1 =  select_survey['mental_health_1'].values\n",
    "mental_health2 =  select_survey['mental_health_2'].values\n",
    "\n",
    "ids = fitbit_survey2['Id'].unique()\n",
    "max_short_day = 0\n",
    "max_long_day = 0\n",
    "for id in ids:\n",
    "    individual_data_short = fitbit_survey2[(fitbit_survey2['Id'] == id) & (fitbit_survey2['survey_date'] < 2)]\n",
    "    individual_data_long = fitbit_survey2[(fitbit_survey2['Id'] == id) & (fitbit_survey2['survey_date'] < 4)]\n",
    "    if len(individual_data_short) > max_short_day:\n",
    "        max_short_day = len(individual_data_short)\n",
    "\n",
    "    if len(individual_data_long) > max_long_day:\n",
    "        max_long_day = len(individual_data_long)\n",
    "\n",
    "# List to store padded matrices for each ID\n",
    "padded_matrices = []\n",
    "\n",
    "for id_val in ids:\n",
    "    # Filter data for the current `id_val` and specified condition\n",
    "    id_data = fitbit_survey2[(fitbit_survey2['Id'] == id_val) & (fitbit_survey2['survey_date'] < 2)]\n",
    "\n",
    "    max_rows = max_short_day\n",
    "    col = id_data.shape[1]\n",
    "    \n",
    "    # Convert to matrix (numpy array) without 'Id' and 'survey_date' columns\n",
    "    id_matrix = id_data.drop(columns=['Id', 'survey_date']).values\n",
    "    \n",
    "    # Pad each matrix to the target shape (max_rows, max_cols)\n",
    "    padded_id_matrix = np.pad(id_matrix, ((0, max_rows - id_matrix.shape[0]), (0, col - id_matrix.shape[1])), mode='constant', constant_values=0) \n",
    "    #pad the dataset with the same length with 0 and the padding value will be ignored by masking\n",
    "    \n",
    "    # Append the padded matrix to the list\n",
    "    padded_matrices.append(padded_id_matrix) #add 2D matrix to 3D\n",
    "\n",
    "# Stack all matrices into a single 3D array (number of IDs, max_rows, max_cols)\n",
    "final_padded_matrix = np.stack(padded_matrices, axis=0) \n",
    "\n",
    "\n",
    "# List to store padded matrices for each ID\n",
    "padded_matrices2 = []\n",
    "\n",
    "for id_val in ids:\n",
    "    # Filter data for the current `id_val` and specified condition\n",
    "    id_data = fitbit_survey2[(fitbit_survey2['Id'] == id_val) & (fitbit_survey2['survey_date'] < 4)]\n",
    "\n",
    "    max_rows = max_long_day\n",
    "    col = id_data.shape[1]\n",
    "    \n",
    "    # Convert to matrix (numpy array) without 'Id' and 'survey_date' columns\n",
    "    id_matrix = id_data.drop(columns=['Id', 'survey_date']).values\n",
    "    \n",
    "    # Pad each matrix to the target shape (max_rows, max_cols)\n",
    "    padded_id_matrix = np.pad(id_matrix, ((0, max_rows - id_matrix.shape[0]), (0, col - id_matrix.shape[1])), mode='constant', constant_values=0) \n",
    "    #pad the dataset with the same length with 0 and the padding value will be ignored by masking\n",
    "    \n",
    "    # Append the padded matrix to the list\n",
    "    padded_matrices2.append(padded_id_matrix) #add 2D matrix to 3D\n",
    "\n",
    "# Stack all matrices into a single 3D array (number of IDs, max_rows, max_cols)\n",
    "final_padded_matrix2 = np.stack(padded_matrices2, axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare matrix for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_short_data = final_padded_matrix.copy()\n",
    "X_long_data = final_padded_matrix2.copy()\n",
    "y_short_data = mental_health1.copy()\n",
    "y_long_data = mental_health2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_reshpaed = X_short_data.reshape(-1, X_short_data.shape[2])\n",
    "X_standardized = scaler.fit_transform(X_reshpaed)\n",
    "X_short_standardized = X_standardized.reshape(X_short_data.shape)\n",
    "\n",
    "X_reshpaed2 = X_long_data.reshape(-1, X_long_data.shape[2])\n",
    "X_standardized2 = scaler.fit_transform(X_reshpaed2)\n",
    "X_short_standardized2 = X_standardized2.reshape(X_long_data.shape) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataX, datay, shuffle = True, train_percentage = 0.7, val_percentage = 0.15):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    dataX: the feature data\n",
    "    datay: the labels\n",
    "    shuffle: whether to shuffle the data before splitting\n",
    "    train_percentage: proportion of data to use for the training set\n",
    "    val_percentage: proportion of data to use for the validation set\n",
    "\n",
    "    returns\n",
    "    data for training, testing, and validating\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random_indices = np.arange(len(dataX))\n",
    "        np.random.shuffle(random_indices)\n",
    "        dataX = dataX[random_indices]\n",
    "        datay = datay[random_indices]\n",
    "    \n",
    "    # Compute split indices\n",
    "    train_end = int(len(dataX) * train_percentage)\n",
    "    val_end = train_end + int(len(dataX) * val_percentage)\n",
    "    \n",
    "    # Split the data\n",
    "    train_X, train_y = dataX[:train_end], datay[:train_end]\n",
    "    val_X, val_y = dataX[train_end:val_end], datay[train_end:val_end]\n",
    "    test_X, test_y = dataX[val_end:], datay[val_end:]\n",
    "    \n",
    "    return train_X, train_y, val_X, val_y, test_X, test_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to training, testing, and validating\n",
    "train_X_data, train_y_data, val_X_data, val_y_data, test_X_data, test_y_data = train_test_split(X_short_standardized, y_short_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyperparameters for CNN-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss: 0.24071228628357252\n",
      "Best Hyperparameters: {'hidden_size': 32, 'num_layers': 3, 'learning_rate': 0.01, 'batch_size': 8}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter space\n",
    "hyperparameter_space = {\n",
    "    'hidden_size': [4, 8, 16, 32, 64],\n",
    "    'num_layers': [1, 2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'batch_size': [4, 8, 16],\n",
    "}\n",
    "\n",
    "# hyperparameter_space = {\n",
    "#     'embed_size': [16, 32, 64],\n",
    "#     'conv_input': [8, 16, 32],\n",
    "#     'input_size': [8, 16, 32],\n",
    "#     'hidden_size': [32, 64, 128],\n",
    "#     'num_layers': [1, 2, 3],\n",
    "#     'batch_size': [16, 32, 64],\n",
    "#     'learning_rate': [0.01, 0.001, 0.0001]\n",
    "#     #'num_epochs': [10, 20, 30] #do I need to tune this hyperparameter\n",
    "# }\n",
    "\n",
    "#transfer data to torch data\n",
    "# Convert training data to PyTorch tensors\n",
    "train_X_new = torch.tensor(train_X_data, dtype=torch.float32)  # Convert train_X to float32 tensor\n",
    "train_y_new = torch.tensor(train_y_data, dtype=torch.float32)  # Convert train_y to float32 tensor\n",
    "\n",
    "# Convert testing data to PyTorch tensors\n",
    "val_X_new = torch.tensor(val_X_data, dtype=torch.float32)    # Convert test_X to float32 tensor\n",
    "val_y_new = torch.tensor(val_y_data, dtype=torch.float32)    # Convert test_y to float32 tensor\n",
    "\n",
    "# Generate synthetic data\n",
    "train_X = train_X_new  # (samples, sequence length, features)\n",
    "train_y = train_y_new        # Corresponding labels\n",
    "train_seq_lengths = torch.randint(1, 110, (100,))  # Sequence lengths\n",
    "\n",
    "val_X = val_X_new\n",
    "val_y = val_y_new\n",
    "val_seq_lengths = torch.randint(1, 110, (20,))\n",
    "\n",
    "# Number of random search trials\n",
    "num_trials = 10\n",
    "best_val_loss = float('inf')\n",
    "best_hyperparams = {}\n",
    "\n",
    "# Random search loop\n",
    "for _ in range(num_trials):\n",
    "    # Randomly sample hyperparameters\n",
    "    hidden_size = random.choice(hyperparameter_space['hidden_size'])\n",
    "    num_layers = random.choice(hyperparameter_space['num_layers'])\n",
    "    learning_rate = random.choice(hyperparameter_space['learning_rate'])\n",
    "    batch_size = random.choice(hyperparameter_space['batch_size'])\n",
    "\n",
    "    # Create model with sampled hyperparameters\n",
    "    model = CNN_LSTM(input_size=39, hidden_size=hidden_size, num_layers=num_layers, output_size=1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training loop with current hyperparameters\n",
    "    num_epochs = 30\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Shuffle training data\n",
    "        random_indices = np.random.permutation(len(train_X))\n",
    "        train_X, train_y, train_seq_lengths = train_X[random_indices], train_y[random_indices], train_seq_lengths[random_indices]\n",
    "\n",
    "        for i in range(0, len(train_X), batch_size):\n",
    "            batch_X = train_X[i:i + batch_size]\n",
    "            batch_y = train_y[i:i + batch_size]\n",
    "            batch_seq_lengths = train_seq_lengths[i:i + batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X, batch_seq_lengths)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_X)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(val_X)):\n",
    "                test_X1 = val_X[i].unsqueeze(0)\n",
    "                test_y1 = val_y[i].unsqueeze(0)\n",
    "                test_seq_length = val_seq_lengths[i].unsqueeze(0)\n",
    "                test_output = model(test_X1, test_seq_length)\n",
    "                test_loss = criterion(test_output, test_y1)\n",
    "                total_test_loss += test_loss.item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(val_X)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "    # Check if current hyperparameters are the best\n",
    "    if avg_test_loss < best_val_loss:\n",
    "        best_val_loss = avg_test_loss\n",
    "        best_hyperparams = {\n",
    "            'hidden_size': hidden_size,\n",
    "            'num_layers': num_layers,\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "\n",
    "# Print best results\n",
    "print(f\"Best Validation Loss: {best_val_loss}\")\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [39, 39, 2], expected input[1, 41, 109] to have 39 channels, but got 41 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 65\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, train_y1)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\EIC-Code\\00-Python\\2024-08-Objective-identification-on-mental-health_CNN_LSTM\\model.py:18\u001b[0m, in \u001b[0;36mCNN_LSTM.forward\u001b[1;34m(self, x, seq_lengths)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, seq_lengths):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Pass through Conv1d layer\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Rearrange for Conv1d: (batch_size, features, seq_len)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Pack the sequence to handle variable lengths\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Rearrange back for LSTM: (batch_size, seq_len, features)\u001b[39;00m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:308\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:304\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    302\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    303\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [39, 39, 2], expected input[1, 41, 109] to have 39 channels, but got 41 channels instead"
     ]
    }
   ],
   "source": [
    "# Define other parameters and hyperparameters\n",
    "input_size = 39  # Feature size\n",
    "hidden_size = 32\n",
    "num_layers = 3\n",
    "batch_size = 8\n",
    "learning_rate = 0.01\n",
    "output_size = 1 \n",
    "num_epochs = 30\n",
    "\n",
    "# Create the CNN_LSTM model instance\n",
    "model = CNN_LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#transfer data to torch data\n",
    "# Convert training data to PyTorch tensors\n",
    "train_X_new = torch.tensor(train_X_data, dtype=torch.float32)  # Convert train_X to float32 tensor\n",
    "train_y_new = torch.tensor(train_y_data, dtype=torch.float32)  # Convert train_y to float32 tensor\n",
    "\n",
    "# Convert testing data to PyTorch tensors\n",
    "test_X_new = torch.tensor(test_X_data, dtype=torch.float32)    # Convert test_X to float32 tensor\n",
    "test_y_new = torch.tensor(test_y_data, dtype=torch.float32)    # Convert test_y to float32 tensor\n",
    "\n",
    "# Generate synthetic data\n",
    "train_X = train_X_new  # (samples, sequence length, features)\n",
    "train_y = train_y_new  # Corresponding labels\n",
    "train_seq_lengths = torch.randint(1, 110, (100,))  # Sequence lengths\n",
    "\n",
    "test_X = test_X_new\n",
    "test_y = test_y_new\n",
    "test_seq_lengths = torch.randint(1, 110, (20,))\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(\"Start Training\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    random_indices = np.random.permutation(len(train_X))\n",
    "    train_X = train_X[random_indices]\n",
    "    train_y = train_y[random_indices]\n",
    "    train_seq_lengths = train_seq_lengths[random_indices]\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(len(train_X)):\n",
    "        # Select a single sample (batch size = 1)\n",
    "        train_X1 = train_X[i].unsqueeze(0)  # Shape (1, seq_len, features)\n",
    "        train_y1 = train_y[i].unsqueeze(0)  # Shape (1, output_size)\n",
    "        seq_length = train_seq_lengths[i].unsqueeze(0)  # Shape (1,)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_X1, seq_length)\n",
    "        loss = criterion(output, train_y1)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_X)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_X)):\n",
    "            test_X1 = test_X[i].unsqueeze(0)\n",
    "            test_y1 = test_y[i].unsqueeze(0)\n",
    "            test_seq_length = test_seq_lengths[i].unsqueeze(0)\n",
    "\n",
    "            test_output = model(test_X1, test_seq_length)\n",
    "            test_loss = criterion(test_output, test_y1)\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_X)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    # Log the progress\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
