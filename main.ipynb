{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from missingpy import MissForest #impute missing value\n",
    "from sklearn.preprocessing import MinMaxScaler #standardized data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#deep learning package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitbit_survey2 = pd.read_csv('fitbit_survey2.csv')\n",
    "select_survey = pd.read_csv('select_survey.csv')\n",
    "\n",
    "fitbit_survey2.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "select_survey.drop(columns = ['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_health1 =  select_survey['mental_health_1'].values\n",
    "mental_health2 =  select_survey['mental_health_2'].values\n",
    "\n",
    "ids = fitbit_survey2['Id'].unique()\n",
    "max_short_day = 0\n",
    "max_long_day = 0\n",
    "for id in ids:\n",
    "    individual_data_short = fitbit_survey2[(fitbit_survey2['Id'] == id) & (fitbit_survey2['survey_date'] < 2)]\n",
    "    individual_data_long = fitbit_survey2[(fitbit_survey2['Id'] == id) & (fitbit_survey2['survey_date'] < 4)]\n",
    "    if len(individual_data_short) > max_short_day:\n",
    "        max_short_day = len(individual_data_short)\n",
    "\n",
    "    if len(individual_data_long) > max_long_day:\n",
    "        max_long_day = len(individual_data_long)\n",
    "\n",
    "# List to store padded matrices for each ID\n",
    "padded_matrices = []\n",
    "\n",
    "for id_val in ids:\n",
    "    # Filter data for the current `id_val` and specified condition\n",
    "    id_data = fitbit_survey2[(fitbit_survey2['Id'] == id_val) & (fitbit_survey2['survey_date'] < 2)]\n",
    "\n",
    "    max_rows = max_short_day\n",
    "    col = id_data.shape[1]\n",
    "    \n",
    "    # Convert to matrix (numpy array) without 'Id' and 'survey_date' columns\n",
    "    id_matrix = id_data.drop(columns=['Id', 'survey_date']).values\n",
    "    \n",
    "    # Pad each matrix to the target shape (max_rows, max_cols)\n",
    "    padded_id_matrix = np.pad(id_matrix, ((0, max_rows - id_matrix.shape[0]), (0, col - id_matrix.shape[1])), mode='constant', constant_values=0) \n",
    "    #pad the dataset with the same length with 0 and the padding value will be ignored by masking\n",
    "    \n",
    "    # Append the padded matrix to the list\n",
    "    padded_matrices.append(padded_id_matrix) #add 2D matrix to 3D\n",
    "\n",
    "# Stack all matrices into a single 3D array (number of IDs, max_rows, max_cols)\n",
    "final_padded_matrix = np.stack(padded_matrices, axis=0) \n",
    "\n",
    "\n",
    "# List to store padded matrices for each ID\n",
    "padded_matrices2 = []\n",
    "\n",
    "for id_val in ids:\n",
    "    # Filter data for the current `id_val` and specified condition\n",
    "    id_data = fitbit_survey2[(fitbit_survey2['Id'] == id_val) & (fitbit_survey2['survey_date'] < 4)]\n",
    "\n",
    "    max_rows = max_long_day\n",
    "    col = id_data.shape[1]\n",
    "    \n",
    "    # Convert to matrix (numpy array) without 'Id' and 'survey_date' columns\n",
    "    id_matrix = id_data.drop(columns=['Id', 'survey_date']).values\n",
    "    \n",
    "    # Pad each matrix to the target shape (max_rows, max_cols)\n",
    "    padded_id_matrix = np.pad(id_matrix, ((0, max_rows - id_matrix.shape[0]), (0, col - id_matrix.shape[1])), mode='constant', constant_values=0) \n",
    "    #pad the dataset with the same length with 0 and the padding value will be ignored by masking\n",
    "    \n",
    "    # Append the padded matrix to the list\n",
    "    padded_matrices2.append(padded_id_matrix) #add 2D matrix to 3D\n",
    "\n",
    "# Stack all matrices into a single 3D array (number of IDs, max_rows, max_cols)\n",
    "final_padded_matrix2 = np.stack(padded_matrices2, axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare matrix for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_short_data = final_padded_matrix.copy()\n",
    "X_long_data = final_padded_matrix2.copy()\n",
    "y_short_data = mental_health1.copy()\n",
    "y_long_data = mental_health2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_reshpaed = X_short_data.reshape(-1, X_short_data.shape[2])\n",
    "X_standardized = scaler.fit_transform(X_reshpaed)\n",
    "X_short_standardized = X_standardized.reshape(X_short_data.shape)\n",
    "\n",
    "X_reshpaed2 = X_long_data.reshape(-1, X_long_data.shape[2])\n",
    "X_standardized2 = scaler.fit_transform(X_reshpaed2)\n",
    "X_short_standardized2 = X_standardized2.reshape(X_long_data.shape) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataX, datay, shuffle = True, train_percentage = 0.7, val_percentage = 0.15):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    dataX: the feature data\n",
    "    datay: the labels\n",
    "    shuffle: whether to shuffle the data before splitting\n",
    "    train_percentage: proportion of data to use for the training set\n",
    "    val_percentage: proportion of data to use for the validation set\n",
    "\n",
    "    returns\n",
    "    data for training, testing, and validating\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random_indices = np.arange(len(dataX))\n",
    "        np.random.shuffle(random_indices)\n",
    "        dataX = dataX[random_indices]\n",
    "        datay = datay[random_indices]\n",
    "    \n",
    "    # Compute split indices\n",
    "    train_end = int(len(dataX) * train_percentage)\n",
    "    val_end = train_end + int(len(dataX) * val_percentage)\n",
    "    \n",
    "    # Split the data\n",
    "    train_X, train_y = dataX[:train_end], datay[:train_end]\n",
    "    val_X, val_y = dataX[train_end:val_end], datay[train_end:val_end]\n",
    "    test_X, test_y = dataX[val_end:], datay[val_end:]\n",
    "    \n",
    "    return train_X, train_y, val_X, val_y, test_X, test_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to training, testing, and validating\n",
    "train_X_data, train_y_data, val_X_data, val_y_data, test_X_data, test_y_data = train_test_split(X_short_standardized, y_short_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyperparameters for CNN-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter space\n",
    "hyperparameter_space = {\n",
    "    'hidden_size': [4, 8, 16, 32, 64],\n",
    "    'num_layers': [1, 2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'batch_size': [4, 8, 16],\n",
    "}\n",
    "\n",
    "# hyperparameter_space = {\n",
    "#     'embed_size': [16, 32, 64],\n",
    "#     'conv_input': [8, 16, 32],\n",
    "#     'input_size': [8, 16, 32],\n",
    "#     'hidden_size': [32, 64, 128],\n",
    "#     'num_layers': [1, 2, 3],\n",
    "#     'batch_size': [16, 32, 64],\n",
    "#     'learning_rate': [0.01, 0.001, 0.0001]\n",
    "#     #'num_epochs': [10, 20, 30] #do I need to tune this hyperparameter\n",
    "# }\n",
    "\n",
    "#transfer data to torch data\n",
    "# Convert training data to PyTorch tensors\n",
    "train_X_new = torch.tensor(train_X_data, dtype=torch.float32)  # Convert train_X to float32 tensor\n",
    "train_y_new = torch.tensor(train_y_data, dtype=torch.float32)  # Convert train_y to float32 tensor\n",
    "\n",
    "# Convert testing data to PyTorch tensors\n",
    "val_X_new = torch.tensor(val_X_data, dtype=torch.float32)    # Convert test_X to float32 tensor\n",
    "val_y_new = torch.tensor(val_y_data, dtype=torch.float32)    # Convert test_y to float32 tensor\n",
    "\n",
    "# Generate synthetic data\n",
    "train_X = train_X_new  # (samples, sequence length, features)\n",
    "train_y = train_y_new        # Corresponding labels\n",
    "train_seq_lengths = torch.randint(1, 110, (100,))  # Sequence lengths\n",
    "\n",
    "val_X = val_X_new\n",
    "val_y = val_y_new\n",
    "val_seq_lengths = torch.randint(1, 110, (20,))\n",
    "\n",
    "# Number of random search trials\n",
    "num_trials = 10\n",
    "best_val_loss = float('inf')\n",
    "best_hyperparams = {}\n",
    "\n",
    "# Random search loop\n",
    "for _ in range(num_trials):\n",
    "    # Randomly sample hyperparameters\n",
    "    hidden_size = random.choice(hyperparameter_space['hidden_size'])\n",
    "    num_layers = random.choice(hyperparameter_space['num_layers'])\n",
    "    learning_rate = random.choice(hyperparameter_space['learning_rate'])\n",
    "    batch_size = random.choice(hyperparameter_space['batch_size'])\n",
    "\n",
    "    # Create model with sampled hyperparameters\n",
    "    model = CNN_LSTM(input_size = 39, hidden_size=hidden_size, num_layers=num_layers, output_size=1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "    \n",
    "    # Training loop with current hyperparameters\n",
    "    num_epochs = 30\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Shuffle training data\n",
    "        random_indices = np.random.permutation(len(train_X))\n",
    "        train_X, train_y, train_seq_lengths = train_X[random_indices], train_y[random_indices], train_seq_lengths[random_indices]\n",
    "\n",
    "        for i in range(0, len(train_X), batch_size):\n",
    "            batch_X = train_X[i:i + batch_size]\n",
    "            batch_y = train_y[i:i + batch_size]\n",
    "            batch_seq_lengths = train_seq_lengths[i:i + batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X, batch_seq_lengths)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_X)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(val_X)):\n",
    "                test_X1 = val_X[i].unsqueeze(0)\n",
    "                test_y1 = val_y[i].unsqueeze(0)\n",
    "                test_seq_length = val_seq_lengths[i].unsqueeze(0)\n",
    "                test_output = model(test_X1, test_seq_length)\n",
    "                test_loss = criterion(test_output, test_y1)\n",
    "                total_test_loss += test_loss.item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(val_X)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "    # Check if current hyperparameters are the best\n",
    "    if avg_test_loss < best_val_loss:\n",
    "        best_val_loss = avg_test_loss\n",
    "        best_hyperparams = {\n",
    "            'hidden_size': hidden_size,\n",
    "            'num_layers': num_layers,\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "\n",
    "# Print best results\n",
    "print(f\"Best Validation Loss: {best_val_loss}\")\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN+LSTM model class\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.conv = nn.Conv1d(input_size, output_size, kernel_size=2) #, activation = 'relu' # Set conv input to match feature size (39)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, seq_lengths):\n",
    "        # Pass through Conv1d layer\n",
    "        x = x.permute(0, 2, 1)  # Rearrange for Conv1d: (batch_size, features, seq_len)\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Pack the sequence to handle variable lengths\n",
    "        x = x.permute(0, 2, 1)  # Rearrange back for LSTM: (batch_size, seq_len, features)\n",
    "        packed_input = pack_padded_sequence(x, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        #print(packed_input.shape)\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        print(h0.shape)\n",
    "        print(c0.shape)\n",
    "        # Pass through LSTM\n",
    "        packed_output, _ = self.lstm(packed_input, (h0, c0))\n",
    "        \n",
    "        # Unpack the sequence\n",
    "        out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Get the output of the last valid time step\n",
    "        out = out[torch.arange(out.size(0)), seq_lengths - 1]\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # Apply softmax\n",
    "        #out = torch.sigmoid(out, dim=1)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters and hyperparameters\n",
    "hidden_size = 32\n",
    "num_layers = 3\n",
    "batch_size = 8\n",
    "learning_rate = 0.01\n",
    "num_epochs = 30\n",
    "filters = 64\n",
    "input_size = 39 #(train_X_new.shape[1], train_X_new.shape[2])  # Feature size, input size is the matrix or only te number of features\n",
    "output_size = 1  #set to 1 (for a single outcput with a sigmoid activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PackedSequence' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 42\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.squeeze() #squeeze to remove extra dimensions\u001b[39;00m\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, train_y1)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\EID-Download\\02 Academic\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[70], line 19\u001b[0m, in \u001b[0;36mCNN_LSTM.forward\u001b[1;34m(self, x, seq_lengths)\u001b[0m\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Rearrange back for LSTM: (batch_size, seq_len, features)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m packed_input \u001b[38;5;241m=\u001b[39m pack_padded_sequence(x, seq_lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpacked_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize hidden and cell states\u001b[39;00m\n\u001b[0;32m     21\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PackedSequence' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# transfer data to torch data\n",
    "# Convert training data to PyTorch tensors\n",
    "train_X_new = torch.tensor(train_X_data, dtype=torch.float32)  # Convert train_X to float32 tensor\n",
    "train_y_new = torch.tensor(train_y_data, dtype=torch.float32)  # Convert train_y to float32 tensor\n",
    "train_seq_lengths = torch.tensor([len(seq) for seq in train_X_data])  # Calculate sequence lengths for training data\n",
    "\n",
    "# Convert testing data to PyTorch tensors\n",
    "test_X_new = torch.tensor(test_X_data, dtype=torch.float32)    # Convert test_X to float32 tensor\n",
    "test_y_new = torch.tensor(test_y_data, dtype=torch.float32)    # Convert test_y to float32 tensor\n",
    "test_seq_lengths = torch.tensor([len(seq) for seq in test_X_data])  # Calculate sequence lengths for test data\n",
    "# Create the CNN_LSTM model instance\n",
    "model = CNN_LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) #betas=(0.5, 0.999)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(\"Start Training\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    random_indices = np.random.permutation(len(train_X_new))\n",
    "    train_X_shuffled  = train_X_new[random_indices]\n",
    "    train_y_shuffled  = train_y_new[random_indices]\n",
    "    train_seq_lengths_shuffled = train_seq_lengths[random_indices]\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(len(train_X_shuffled)):\n",
    "        # Select a single sample (batch size = 1)\n",
    "        train_X1 = train_X_shuffled[i].unsqueeze(0)  # Shape (1, seq_len, features)\n",
    "        train_y1 = train_y_shuffled[i].unsqueeze(0)  # Shape (1, output_size)\n",
    "        seq_length = train_seq_lengths_shuffled[i].unsqueeze(0)  # Shape (1, )\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_X1, seq_length)#.squeeze() #squeeze to remove extra dimensions\n",
    "        loss = criterion(output, train_y1)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_X)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_X_new)):\n",
    "            test_X1 = test_X_new[i].unsqueeze(0)\n",
    "            test_y1 = test_X_new[i].unsqueeze(0)\n",
    "            test_seq_length = test_seq_lengths[i].unsqueeze(0) \n",
    "\n",
    "            test_output = model(test_X1, test_seq_length)#.squeeze()\n",
    "            test_loss = criterion(test_output, test_y1)\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_X_new)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    # Log the progress\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 109, 39])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
